{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003ca320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "from keras.models import load_model  # TensorFlow is required for Keras to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f4dcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3df57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the prediction from loaded model\n",
    "model = load_model(\"pages/TM/model.savedmodel\", compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25cbf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the labels\n",
    "class_names = open(\"pages/TM/TM_labels.txt\", \"r\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922a68cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the array of the right shape to feed into the keras model\n",
    "# The 'length' or number of images you can put into the array is\n",
    "# determined by the first position in the shape tuple, in this case 1\n",
    "data = np.ndarray(shape=(1, 224, 224, 3), dtype=np.float32)\n",
    "\n",
    "# Replace this with the path to your image\n",
    "image = Image.open(\"pages/TM/B_test2.jpg\").convert(\"RGB\")\n",
    "\n",
    "# resizing the image to be at least 224x224 and then cropping from the center\n",
    "size = (224, 224)\n",
    "image = ImageOps.fit(image, size, Image.Resampling.LANCZOS)\n",
    "\n",
    "# turn the image into a numpy array\n",
    "image_array = np.asarray(image)\n",
    "\n",
    "# Normalize the image\n",
    "normalized_image_array = (image_array.astype(np.float32) / 127.5) - 1\n",
    "\n",
    "# Load the image into the array\n",
    "data[0] = normalized_image_array\n",
    "\n",
    "# Predicts the model\n",
    "prediction = model.predict(data)\n",
    "index = np.argmax(prediction)\n",
    "class_name = class_names[index]\n",
    "confidence_score = prediction[0][index]\n",
    "\n",
    "# Print prediction and confidence score\n",
    "print(\"Letter:\", class_name[2:], end=\"\")\n",
    "print(\"Confidence Score:\", confidence_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e1103c",
   "metadata": {},
   "source": [
    "## testing the model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a20a5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfa6ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"generated_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2317558",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = image_dataset_from_directory(\n",
    "  path,\n",
    "  labels = \"inferred\",\n",
    "  label_mode = \"categorical\",\n",
    "  seed=42,\n",
    "  image_size=(200, 200),\n",
    "  batch_size=16,\n",
    "  validation_split=0.3,\n",
    "  subset = 'training')\n",
    "\n",
    "# We define a second one for the test data\n",
    "\n",
    "validation_ds = image_dataset_from_directory(\n",
    "  path,\n",
    "  labels = \"inferred\",\n",
    "  label_mode = \"categorical\",\n",
    "  seed=42,\n",
    "  image_size=(200, 200),\n",
    "  batch_size=16,\n",
    "  validation_split=0.3,\n",
    "  subset = 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7a1792",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29604d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558b78fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"model_1\"\n",
    "\n",
    "modelCheckpooint = callbacks.ModelCheckpoint(\"{}.h5\".format(MODEL), monitor=\"val_loss\", verbose=0, save_best_only=True)\n",
    "\n",
    "LRreducer = callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor = 0.1, patience=3, verbose=1, min_lr=0)\n",
    "\n",
    "EarlyStopper = callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=0, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dbf43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(layers.Rescaling(1./255, input_shape = (200, 200, 3)))\n",
    "model.add(layers.Conv2D(filters = 32, kernel_size = (3,3), activation=\"relu\", padding = \"same\"))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2), padding = \"same\") )\n",
    "\n",
    "\n",
    "model.add(layers.Conv2D(filters = 32, kernel_size = (3,3), activation=\"relu\", padding = \"same\"))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2), padding = \"same\") )\n",
    "\n",
    "\n",
    "model.add(layers.Conv2D(filters = 64, kernel_size = (3,3), activation=\"relu\", padding = \"same\"))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2), padding = \"same\") )\n",
    "\n",
    "model.add(layers.Conv2D(filters = 128, kernel_size = (3,3), activation=\"relu\", padding = \"same\"))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2), padding = \"same\") )\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# Here we flatten our data to end up with just one dimension\n",
    "\n",
    "model.add(layers.Dense(64, activation=\"relu\"))\n",
    "\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(layers.Dense(26, activation=\"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ae3acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = optimizers.Adam(learning_rate = 0.001)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer= adam,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311b4df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "        train_ds,\n",
    "        epochs=40,\n",
    "        validation_data=validation_ds,\n",
    "        batch_size = 64,\n",
    "        callbacks = [modelCheckpooint, LRreducer, EarlyStopper])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50964295",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.saving import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3b1433",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_imported = load_model('model_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd93dae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = cv2.imread('generated_images/A_annotated_image.jpg')\n",
    "plt.imshow(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0674a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c064ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_exp = np.expand_dims(test_image, axis=0)\n",
    "test_image_exp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c5b2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.zeros(26)\n",
    "y_test[0] = 1 #as the test image is A\n",
    "y_image_exp = np.expand_dims(y_test, axis=0)\n",
    "y_image_exp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c5dbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_imported.evaluate(test_image_exp, y_image_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a87fed",
   "metadata": {},
   "source": [
    "# Update jpg image with landmarks and crop it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed30b90f",
   "metadata": {},
   "source": [
    "### first step we display the input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5e3307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0de6a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('raw_data/Kaggle2/data/C/C192.jpg')\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0e1d94",
   "metadata": {},
   "source": [
    "### add landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999f4ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Hand module\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands.Hands()\n",
    "\n",
    "# Convert image to RGB and process with Hand module\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "results = mp_hands.process(image_rgb)\n",
    "\n",
    "# Draw landmarks on the image\n",
    "if results.multi_hand_landmarks:\n",
    "    for hand_landmarks in results.multi_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image,\n",
    "            hand_landmarks,\n",
    "            mp.solutions.hands.HAND_CONNECTIONS,\n",
    "            mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=2, circle_radius=2),\n",
    "            mp_drawing.DrawingSpec(color=(255, 255, 255), thickness=2)\n",
    "        )\n",
    "\n",
    "# Save the image with landmarks\n",
    "cv2.imwrite('raw_data/landmarked.jpg', image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632f1307",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_landmark = cv2.imread('raw_data/landmarked.jpg')\n",
    "plt.imshow(image_landmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c2425c",
   "metadata": {},
   "source": [
    "### crop the image based on landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa6d8d24",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_landmark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m max_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(y_coordinates)\u001b[38;5;241m+\u001b[39m_extend,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# crop the image\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m cropped_image \u001b[38;5;241m=\u001b[39m \u001b[43mimage_landmark\u001b[49m[\u001b[38;5;28mint\u001b[39m(min_y \u001b[38;5;241m*\u001b[39m image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\u001b[38;5;28mint\u001b[39m(max_y \u001b[38;5;241m*\u001b[39m image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;28mint\u001b[39m(min_x \u001b[38;5;241m*\u001b[39m image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\u001b[38;5;28mint\u001b[39m(max_x \u001b[38;5;241m*\u001b[39m image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])]\n\u001b[1;32m     19\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw_data/landmarked_cropped.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, cropped_image)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image_landmark' is not defined"
     ]
    }
   ],
   "source": [
    "landmark_list = []\n",
    "if results.multi_hand_landmarks:\n",
    "    for hand_landmarks in results.multi_hand_landmarks:\n",
    "        for landmark in hand_landmarks.landmark:\n",
    "            landmark_list.append((landmark.x, landmark.y))\n",
    "            \n",
    "#calculate the bounding box\n",
    "x_coordinates = [landmark[0] for landmark in landmark_list]\n",
    "y_coordinates = [landmark[1] for landmark in landmark_list]\n",
    "_extend = 0.1\n",
    "min_x = max(min(x_coordinates)-_extend,0)\n",
    "max_x = min(max(x_coordinates)+_extend,1)\n",
    "min_y = max(min(y_coordinates)-_extend,0)\n",
    "max_y = min(max(y_coordinates)+_extend,1)\n",
    "\n",
    "# crop the image\n",
    "cropped_image = image_landmark[int(min_y * image.shape[0]):int(max_y * image.shape[0]), int(min_x * image.shape[1]):int(max_x * image.shape[1])]\n",
    "\n",
    "cv2.imwrite('raw_data/landmarked_cropped.jpg', cropped_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89ce98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_landmark_cropped = cv2.imread('raw_data/landmarked_cropped.jpg')\n",
    "plt.imshow(image_landmark_cropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84129f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_landmark_cropped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6481df",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_landmark_cropped_resize = cv2.resize(image_landmark_cropped, (300, 300))\n",
    "plt.imshow(image_landmark_cropped_resize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25f898e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_landmark_cropped_resize.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af289489",
   "metadata": {},
   "source": [
    "### making a loop for couple of folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "744ce543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import gc\n",
    "import time\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62f4ae46",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: 'raw_data/Kaggle2/landmarked/J'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw_data/Kaggle2/landmarked\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m folder \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(path):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m#create a new folder for future output\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, folder)):\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: 'raw_data/Kaggle2/landmarked/J'"
     ]
    }
   ],
   "source": [
    "path = 'raw_data/Kaggle2/data'\n",
    "output_path = 'raw_data/Kaggle2/landmarked'\n",
    "for folder in os.listdir(path):\n",
    "    #create a new folder for future output\n",
    "    os.mkdir(os.path.join(output_path, folder))\n",
    "    for file in os.listdir(os.path.join(path, folder)):\n",
    "        if file.endswith(\".jpg\"):\n",
    "            \n",
    "            image = cv2.imread(os.path.join(path, os.path.join(folder, file)))\n",
    "\n",
    "            # Initialize MediaPipe Hand module\n",
    "            mp_drawing = mp.solutions.drawing_utils\n",
    "            mp_hands = mp.solutions.hands.Hands()\n",
    "\n",
    "            # Convert image to RGB and process with Hand module\n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            results = mp_hands.process(image_rgb)\n",
    "\n",
    "            # Draw landmarks on the image\n",
    "            landmark_list = []\n",
    "            if results.multi_hand_landmarks:\n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                     mp_drawing.draw_landmarks(\n",
    "                        image,\n",
    "                        hand_landmarks,\n",
    "                        mp.solutions.hands.HAND_CONNECTIONS,\n",
    "                        mp_drawing.DrawingSpec(color = (0, 0, 255), thickness=2, circle_radius=2),\n",
    "                        mp_drawing.DrawingSpec(color = (255, 255, 255), thickness=2))\n",
    "                     for landmark in hand_landmarks.landmark:\n",
    "                        landmark_list.append((landmark.x, landmark.y))\n",
    "\n",
    "            # QC the landmark setting process and skip image if did not work\n",
    "            if len(landmark_list) < 1: continue\n",
    "                \n",
    "            #calculate the bounding box\n",
    "            x_coordinates = [landmark[0] for landmark in landmark_list]\n",
    "            y_coordinates = [landmark[1] for landmark in landmark_list]\n",
    "            \n",
    "            # extending, yet making sure it's within [0,1] canvas\n",
    "            _extend = 0.1\n",
    "            min_x = max(min(x_coordinates)-_extend,0)\n",
    "            max_x = min(max(x_coordinates)+_extend,1)\n",
    "            min_y = max(min(y_coordinates)-_extend,0)\n",
    "            max_y = min(max(y_coordinates)+_extend,1)\n",
    "\n",
    "            # crop the image\n",
    "            cropped_image = image[int(min_y * image.shape[0]):int(max_y * image.shape[0]), int(min_x * image.shape[1]):int(max_x * image.shape[1])]\n",
    "\n",
    "            if np.logical_and.reduce([cropped_image.shape[0]>=50, cropped_image.shape[1]>=50, len(landmark_list)>15]):\n",
    "                image_export = cv2.resize(cropped_image, (300, 300))\n",
    "                cv2.imwrite(f'raw_data/Kaggle2/landmarked/{folder}/{file}', image_export)\n",
    "                gc.collect()\n",
    "            else:\n",
    "                count+=1\n",
    "                print(\"mising\", count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcbc76d",
   "metadata": {},
   "source": [
    "### looks like have to go  folder by folder(("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4991017e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14min 26s, sys: 16.2 s, total: 14min 42s\n",
      "Wall time: 14min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "folder_list = ['H', 'I']\n",
    "path = 'raw_data/Kaggle2/data'\n",
    "output_path = 'raw_data/Kaggle2/landmarked'\n",
    "\n",
    "# Initialize MediaPipe Hand module\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands.Hands()\n",
    "\n",
    "for folder in folder_list:\n",
    "    os.mkdir(os.path.join(output_path, folder))\n",
    "    for file in os.listdir(os.path.join(path, folder)):\n",
    "        if file.endswith(\".jpg\"):\n",
    "            \n",
    "            image = cv2.imread(os.path.join(os.path.join(path, folder), file))\n",
    "\n",
    "            # Convert image to RGB and process with Hand module\n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            results = mp_hands.process(image_rgb)\n",
    "\n",
    "            # Draw landmarks on the image\n",
    "            landmark_list = []\n",
    "            if results.multi_hand_landmarks:\n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                     mp_drawing.draw_landmarks(\n",
    "                        image,\n",
    "                        hand_landmarks,\n",
    "                        mp.solutions.hands.HAND_CONNECTIONS,\n",
    "                        mp_drawing.DrawingSpec(color = (0, 0, 255), thickness=2, circle_radius=2),\n",
    "                        mp_drawing.DrawingSpec(color = (255, 255, 255), thickness=2))\n",
    "                     for landmark in hand_landmarks.landmark:\n",
    "                        landmark_list.append((landmark.x, landmark.y))\n",
    "\n",
    "            # QC the landmark setting process and skip image if did not work\n",
    "            if len(landmark_list) < 1: continue\n",
    "                \n",
    "            #calculate the bounding box\n",
    "            x_coordinates = [landmark[0] for landmark in landmark_list]\n",
    "            y_coordinates = [landmark[1] for landmark in landmark_list]\n",
    "            \n",
    "            # extending, yet making sure it's within [0,1] canvas\n",
    "            _extend = 0.1\n",
    "            min_x = max(min(x_coordinates)-_extend,0)\n",
    "            max_x = min(max(x_coordinates)+_extend,1)\n",
    "            min_y = max(min(y_coordinates)-_extend,0)\n",
    "            max_y = min(max(y_coordinates)+_extend,1)\n",
    "\n",
    "            # crop the image\n",
    "            cropped_image = image[int(min_y * image.shape[0]):int(max_y * image.shape[0]), int(min_x * image.shape[1]):int(max_x * image.shape[1])]\n",
    "\n",
    "            if np.logical_and.reduce([cropped_image.shape[0]>=50, cropped_image.shape[1]>=50, len(landmark_list)>15]):\n",
    "                image_export = cv2.resize(cropped_image, (300, 300))\n",
    "                cv2.imwrite(f'{os.path.join(output_path, folder)}/{file}', image_export)\n",
    "                gc.collect()\n",
    "#                 time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b1402a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
